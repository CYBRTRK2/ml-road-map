[<< Lecture 1](lecture_1.md) • [Lecture 3 >>](lecture_3.md)
## Lecture 2 - Linear Regression and Gradient Descent

[![Stanford CS229: Machine Learning - Linear Regression and Gradient Descent | Lecture 2 (Autumn 2018)](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D4b4MUYve_U8%26list%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)](https://www.youtube.com/watch?v=4b4MUYve_U8&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)

### Topics

* Linear Regression
* Supervised Learning
* Normal Equations
* Hypotheses
* Features
* Cost Functions

### Timestamps

* [0:29](https://youtu.be/4b4MUYve_U8?si=k0wMbM5auk-E3GLx&t=29): 📚 The video discusses linear regression as a learning algorithm for fitting linear regression models.
* [5:43](https://youtu.be/4b4MUYve_U8?si=k0wMbM5auk-E3GLx&t=343): ✏️ The video discusses using multiple input features to estimate the size of a house in real estate.
* [12:03](https://youtu.be/4b4MUYve_U8?si=k0wMbM5auk-E3GLx&t=723): 📚 The video discusses the features and parameters in a learning algorithm and how to choose the parameters.
* [18:40](https://youtu.be/4b4MUYve_U8?si=k0wMbM5auk-E3GLx&t=1120): 📉 Gradient descent is a method used to minimize a function by iteratively adjusting the values of Theta.
* [24:43](https://youtu.be/4b4MUYve_U8?si=k0wMbM5auk-E3GLx&t=1483): 📚 The video discusses the concept of gradient descent and its application in updating values.
* [30:13](https://youtu.be/4b4MUYve_U8?si=k0wMbM5auk-E3GLx&t=1813): 📚 The video explains the concept of partial derivatives and its application in gradient descent.
* [36:08](https://youtu.be/4b4MUYve_U8?si=k0wMbM5auk-E3GLx&t=2168): ⚡ Choosing the right learning rate is crucial for convergence in gradient descent.
* [41:35](https://youtu.be/4b4MUYve_U8?si=k0wMbM5auk-E3GLx&t=2495): 📚 Batch gradient descent is a method in machine learning where the entire training set is processed as one batch of data.
* [47:13](https://youtu.be/4b4MUYve_U8?si=k0wMbM5auk-E3GLx&t=2833): 📚 Stochastic gradient descent allows for faster progress in large data-sets compared to batch gradient descent.
* [52:2](https://youtu.be/4b4MUYve_U8?si=k0wMbM5auk-E3GLx&t=3143): 💡 Gradient descent is an iterative algorithm used in various machine learning algorithms, but for linear regression, the normal equation can be used to find the global optimum in one step.
* [59:09](https://youtu.be/4b4MUYve_U8?si=k0wMbM5auk-E3GLx&t=3549): 📚 The video explains how to calculate the derivative of a matrix function with respect to the matrix itself.
* [1:05:51](https://youtu.be/4b4MUYve_U8?si=k0wMbM5auk-E3GLx&t=3951): 💡 The trace of AB is equal to the trace of BA.
* [1:13:52](https://youtu.be/4b4MUYve_U8?si=k0wMbM5auk-E3GLx&t=4432): 💡 The video discusses the steps to find the derivative of a quadratic function.
  
Timestamps by Tammy AI

[<< Lecture 1](lecture_1.md) • [Lecture 3 >>](lecture_3.md)