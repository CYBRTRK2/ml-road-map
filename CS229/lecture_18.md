[<< Lecture 17](lecture_17.md) ‚Ä¢ [Lecture 19 >>](lecture_19.md)
## Lecture 18 - Continous State Markov Decision Processes and Model Simulation

[![Lecture 18 - Continous State MDP & Model Simulation | Stanford CS229: Machine Learning (Autumn 2018)](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DQFu5nuc-S0s%26list%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU%26index%3D18)](https://www.youtube.com/watch?v=QFu5nuc-S0s&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=18)

### Overview

* Continuous State Markov Decision Processes
* Modeling Continuous State Markov Decision Processes
* Fitted Value Iteration

### Timestamps
  
[0:18](https://youtu.be/QFu5nuc-S0s?si=pACXcX8MMJ98rUzL&t=18): üéì Application of reinforcement learning to continuous state MDPs and model-based RL.  
[5:43](https://youtu.be/QFu5nuc-S0s?si=pACXcX8MMJ98rUzL&t=343): üöÅ Modeling state-space of vehicles: cars and helicopters in 3D.  
[10:55](https://youtu.be/QFu5nuc-S0s?si=pACXcX8MMJ98rUzL&t=655): üîÑ Discretization simplifies continuous state problems in MDPs by converting them to discrete state problems, making it reasonable for small, low-dimensional MDPs.  
[17:19](https://youtu.be/QFu5nuc-S0s?si=pACXcX8MMJ98rUzL&t=1039): ‚öôÔ∏è Discretization of state space is effective for small to medium dimensions, with careful selection for higher dimensions.  
[23:50](https://youtu.be/QFu5nuc-S0s?si=pACXcX8MMJ98rUzL&t=1430): üéì Introduction to State Transition Probabilities and Model Building in MDPs  
[29:53](https://youtu.be/QFu5nuc-S0s?si=pACXcX8MMJ98rUzL&t=1793): üöÅ Helicopter autonomous controller development process explained using GPS and accelerometers.  
[36:01](https://youtu.be/QFu5nuc-S0s?si=pACXcX8MMJ98rUzL&t=2161): üöÅ Linear regression model can be used for understanding helicopter dynamics and fuel consumption in machine learning.  
[41:31](https://youtu.be/QFu5nuc-S0s?si=pACXcX8MMJ98rUzL&t=2491): ‚öôÔ∏è Importance of Model-Based RL in Robotics  
[46:37](https://youtu.be/QFu5nuc-S0s?si=pACXcX8MMJ98rUzL&t=2797): ‚öôÔ∏è Approximating v-star using a function v of s with nonlinear features to represent expected payoff and robot performance.  
[54:28](https://youtu.be/QFu5nuc-S0s?si=pACXcX8MMJ98rUzL&t=3268): üìä Estimating Expected Value through Sampling in MDP  
[1:03:06](https://youtu.be/QFu5nuc-S0s?si=pACXcX8MMJ98rUzL&t=3786): üé≤ Using deterministic simulator simplifies algorithm by sampling once instead of k times.  
[1:09:45](https://youtu.be/QFu5nuc-S0s?si=pACXcX8MMJ98rUzL&t=4185): ‚öôÔ∏è Implementation of Deep Reinforcement Learning and Regression Algorithms  
[1:16:52](https://youtu.be/QFu5nuc-S0s?si=pACXcX8MMJ98rUzL&t=4612): ‚öôÔ∏è Implementing real-time simulation using a deterministic simulator to compute and apply possible actions based on learned value functions.

Timestamps by Tammy AI

[<< Lecture 17](lecture_17.md) ‚Ä¢ [Lecture 19 >>](lecture_19.md)