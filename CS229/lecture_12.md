[<< Lecture 11](lecture_11.md) â€¢ [Lecture 13 >>](lecture_13.md)
## Lecture 12 - Backpropagation and Improving Neural Networks

[![Lecture 12 - Backprop & Improving Neural Networks | Stanford CS229: Machine Learning (Autumn 2018)](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DzUazLXZZA2U%26list%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU%26index%3D12)](https://www.youtube.com/watch?v=zUazLXZZA2U&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=12)

### Topics

* Backpropagation
* Improving Neural Networks
* Logistic Regression as a One-Neuron Neural Network
* Activation Functions
* Neural Networks with Multiple Layers
* Forward Propagation

### Timestamps
  
[0:38](https://youtu.be/zUazLXZZA2U?si=SBFJ9eCx6dn028gU&t=38): ðŸŽ“ The second lecture on deep learning for CS229 covers logistic regression as a one-neuron neural network and introduces the concept of stacking neurons and layers.  
[6:18](https://youtu.be/zUazLXZZA2U?si=SBFJ9eCx6dn028gU&t=378): ðŸ’¡ The video explains the process of taking the derivative of a loss function with respect to a weight in a neural network.  
[13:31](https://youtu.be/zUazLXZZA2U?si=SBFJ9eCx6dn028gU&t=811): ðŸ“š The video explains the process of taking derivatives in higher dimensions and the importance of analyzing shapes.  
[20:32](https://youtu.be/zUazLXZZA2U?si=SBFJ9eCx6dn028gU&t=1232): ðŸŽ¯ The video explains the process of backpropagation in neural networks.  
[27:01](https://youtu.be/zUazLXZZA2U?si=SBFJ9eCx6dn028gU&t=1621): ðŸ“š The video discusses the process of computing derivatives in a neural network.  
[32:43](https://youtu.be/zUazLXZZA2U?si=SBFJ9eCx6dn028gU&t=1963): ðŸ’¡ During forward propagation, values including weights are saved in memory to be used during backward propagation for computational efficiency.  
[40:05](https://youtu.be/zUazLXZZA2U?si=SBFJ9eCx6dn028gU&t=2405): ðŸ¤” Activation functions are used to determine the output of a neural network and can be chosen based on the desired range of values.  
[49:09](https://youtu.be/zUazLXZZA2U?si=SBFJ9eCx6dn028gU&t=2949): ðŸ“Š Normalization of input can help avoid saturation of the network caused by large or low negative values.  
[55:02](https://youtu.be/zUazLXZZA2U?si=SBFJ9eCx6dn028gU&t=3302): ðŸ“Š The video discusses the computation of y hats in a neural network and the consequences of using larger w_l matrices.  
[1:02:11](https://youtu.be/zUazLXZZA2U?si=SBFJ9eCx6dn028gU&t=3731): ðŸ§  Different weight initialization techniques in neural networks.  
[1:10:09](https://youtu.be/zUazLXZZA2U?si=SBFJ9eCx6dn028gU&t=42090): ðŸ“Š Stochastic gradient descent and batch gradient descent are two algorithms used for optimization in machine learning, with stochastic gradient descent being faster but noisier.  

Timestamps by Tammy AI

[<< Lecture 11](lecture_11.md) â€¢ [Lecture 13 >>](lecture_13.md)