[<< Lecture 10](lecture_10.md) ‚Ä¢ [Lecture 12 >>](lecture_12.md)
## Lecture 11 - Introduction to Neural Networks

[![Lecture 11 - Introduction to Neural Networks | Stanford CS229: Machine Learning (Autumn 2018)](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DMfIjxPh6Pys%26list%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU%26index%3D11)](https://www.youtube.com/watch?v=MfIjxPh6Pys&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=11)

### Topics

* Improving Neural Networks
* Parametric and Non-Parametric Learning
* Neural Networks

### Timestamps
  
[0:33](https://youtu.be/MfIjxPh6Pys?si=pF3669YTJXyGws-n&t=33): üìö The video introduces deep learning and neural networks, starting with logistic regression as a specific case of a neural network.  
[6:48](https://youtu.be/MfIjxPh6Pys?si=pF3669YTJXyGws-n&t=408): üéØ The video discusses the training process for logistic regression models.  
[14:09](https://youtu.be/MfIjxPh6Pys?si=pF3669YTJXyGws-n&t=849): üêà The goal is to detect images of cats, lions, and iguanas using a modified network.  
[21:47](https://youtu.be/MfIjxPh6Pys?si=pF3669YTJXyGws-n&t=1307): ü¶Å Neural networks can be robust to incorrect labeling if they have enough data to learn from.  
[28:22](https://youtu.be/MfIjxPh6Pys?si=pF3669YTJXyGws-n&t=1702): üìä The formula for calculating the output of a neural network is interesting because it requires the outputs to sum up to 1, creating a probability distribution over all the classes.  
[35:34](https://youtu.be/MfIjxPh6Pys?si=pF3669YTJXyGws-n&t=2134): üìö The video introduces the softmax cross entropy loss function used in deep learning for classification problems.  
[43:36](https://youtu.be/MfIjxPh6Pys?si=pF3669YTJXyGws-n&t=2616): üß† The video discusses the number of parameters in a neural network and defines key vocabulary related to layers.  
[51:24](https://youtu.be/MfIjxPh6Pys?si=pF3669YTJXyGws-n&t=3084): üìù The video explains the equations used to propagate inputs through a neural network using matrices.  
[59:58](https://youtu.be/MfIjxPh6Pys?si=pF3669YTJXyGws-n&t=3598): ‚ùì The video discusses an algebraic problem related to matrix addition and broadcasting in parallel computing.  
[1:14:13](https://youtu.be/MfIjxPh6Pys?si=pF3669YTJXyGws-n&t=4453): üîÅ Backward propagation is used to compute the derivative of the loss function with respect to the weights in a neural network.  

Timestamps by Tammy AI

[<< Lecture 10](lecture_10.md) ‚Ä¢ [Lecture 12 >>](lecture_12.md)