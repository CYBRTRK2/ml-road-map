[<< Lecture 5](lecture_5.md) â€¢ [Lecture 7 >>](lecture_7.md)
## Lecture 6 - Support Vector Machines

[![Lecture 6 - Support Vector Machines | Stanford CS229: Machine Learning Andrew Ng (Autumn 2018)](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DlDwow4aOrtg%26list%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU%26index%3D6)](https://www.youtube.com/watch?v=lDwow4aOrtg&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=6)

### Topics

* Naive Bayes and Generative Learning Algorithms
* Spam Classification
* Decision Boundaries and Margins
* Geometric Margin
* Optimization for Support Vector Machines

### Timestamps
  
[0:24](https://youtu.be/lDwow4aOrtg?si=svcDq4i8cP3euF-j&t=24): ðŸ“š The video discusses the use of Naive Bayes and Laplace smoothing in building a spam classifier and introduces an even better version of Naive Bayes.  
[6:50](https://youtu.be/lDwow4aOrtg?si=svcDq4i8cP3euF-j&t=410): ! The speaker discusses the Naive Bayes algorithm and the importance of not assuming a probability of 0 for unseen events.  
[12:53](https://youtu.be/lDwow4aOrtg?si=svcDq4i8cP3euF-j&t=773): ðŸŒž Laplace's estimation of the chance of the sun rising tomorrow based on historical data.  
[18:52](https://youtu.be/lDwow4aOrtg?si=svcDq4i8cP3euF-j&t=1132): ðŸ’¡ Naive Bayes can be used as a multinomial probability estimator for problems with multiple outcomes and discretized variables.  
[25:25](https://youtu.be/lDwow4aOrtg?si=svcDq4i8cP3euF-j&t=1525): ! The video discusses the Naive Bayes assumption and the Multinomial Event Model in machine learning.  
[31:37](https://youtu.be/lDwow4aOrtg?si=svcDq4i8cP3euF-j&t=1897): ðŸ“š The video discusses how to calculate the ratio of a specific word in an email and implement Laplace smoothing.  
[37:14](https://youtu.be/lDwow4aOrtg?si=svcDq4i8cP3euF-j&t=2234): ðŸ§  Start by implementing something quickly and train the algorithm, then iterate and improve based on performance and error analysis.  
[42:13](https://youtu.be/lDwow4aOrtg?si=svcDq4i8cP3euF-j&t=2533): ðŸ’¡ Gaussian discriminant analysis and Naive Bayes are quick and simple algorithms, but not the most accurate for classification.  
[48:14](https://youtu.be/lDwow4aOrtg?si=svcDq4i8cP3euF-j&t=2894): ðŸ§  Support Vector Machines (SVM) can derive an algorithm to map input features to a higher dimensional set of features and learn non-linear decision boundaries.  
[55:01](https://youtu.be/lDwow4aOrtg?si=svcDq4i8cP3euF-j&t=3301): ðŸ“š The functional margin of a classifier is a measure of how confidently and accurately it classifies examples.  
[1:00:50](https://youtu.be/lDwow4aOrtg?si=svcDq4i8cP3euF-j&t=3650): ðŸ“š The support vector machine (SVM) outputs either -1 or +1 as class labels and has a hard transition between them.  
[1:08:01](https://youtu.be/lDwow4aOrtg?si=svcDq4i8cP3euF-j&t=4081): ðŸ’¡ The functional margin measures how well a model is doing on the worst example in the training set.  
[1:14:32](https://youtu.be/lDwow4aOrtg?si=svcDq4i8cP3euF-j&t=4472): ðŸ“š The video explains the formula for measuring the Euclidean distance and the relationship between the geometric margin and the functional margin.  
  
Timestamps by Tammy AI

[<< Lecture 5](lecture_5.md) â€¢ [Lecture 7 >>](lecture_7.md)