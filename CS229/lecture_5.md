[<< Lecture 4](lecture_4.md) â€¢ [Lecture 6 >>](lecture_6.md)
## Lecture 5 - GDA and Naive Bayes

[![Lecture 5 - GDA & Naive Bayes | Stanford CS229: Machine Learning Andrew Ng (Autumn 2018)](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dnt63k3bfXS0%26list%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU%26index%3D5)](https://www.youtube.com/watch?v=nt63k3bfXS0&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=5)

### Overview

### Timestamps
  
[0:22](https://youtu.be/nt63k3bfXS0?si=0Ugk431WawZMmumD&t=22): ðŸŽ“ The video discusses generative learning algorithms, specifically Gaussian discriminant analysis, and its advantages over logistic regression.  
[6:16](https://youtu.be/nt63k3bfXS0?si=0Ugk431WawZMmumD&t=376): ðŸ§  Bayes' Rule can be used to calculate the probability of a tumor being malignant or benign based on features.  
[12:04](https://youtu.be/nt63k3bfXS0?si=0Ugk431WawZMmumD&t=724): ðŸ“Š The video explains the concept of Multivariate Gaussian density and how it is represented by mean and covariance parameters.  
[16:52](https://youtu.be/nt63k3bfXS0?si=0Ugk431WawZMmumD&t=1012): ðŸ“Š Principal components analysis is used to find the eigenvectors of the covariance matrix.  
[23:19](https://youtu.be/nt63k3bfXS0?si=0Ugk431WawZMmumD&t=1399): ðŸ“š Generative learning algorithms maximize the joint likelihood, while discriminative learning algorithms maximize the conditional likelihood.  
[30:50](https://youtu.be/nt63k3bfXS0?si=0Ugk431WawZMmumD&t=1850): ! The video explains the calculation of mean and covariance for feature vectors in a training set.  
[37:23](https://youtu.be/nt63k3bfXS0?si=0Ugk431WawZMmumD&t=2243): ðŸŽ¯ Logistic regression is a discriminative learning algorithm that searches for a line to separate positive and negative examples, while Gaussian discriminant analysis is a generative learning algorithm that fits Gaussians to the positive and negative examples.  
[44:30](https://youtu.be/nt63k3bfXS0?si=0Ugk431WawZMmumD&t=2670): ðŸ“Š The video discusses fitting Gaussian distributions to two different data sets and exploring the probability of a given class for different values of X.  
[51:04](https://youtu.be/nt63k3bfXS0?si=0Ugk431WawZMmumD&t=3064): ðŸ“Š The video discusses the assumptions and implications of logistic regression and the generative learning algorithm.
[56:31](https://youtu.be/nt63k3bfXS0?si=0Ugk431WawZMmumD&t=3391): ðŸ’¡ Algorithms can be trained with less knowledge about the world by using logistic regression and big data.  
[1:00:51](https://youtu.be/nt63k3bfXS0?si=0Ugk431WawZMmumD&t=3651): ðŸ“Š The skill level of a team and the assumptions made in the algorithm design play a significant role in the performance difference between small and large data sets in machine learning.  
[1:12:36](https://youtu.be/nt63k3bfXS0?si=0Ugk431WawZMmumD&t=4356): ðŸ“š Naive Bayes is a conditional probability model that assumes the independence of words in a text.
  
Timestamps by Tammy AI

[<< Lecture 4](lecture_4.md) â€¢ [Lecture 6 >>](lecture_6.md)