[<< Lecture 15](lecture_15.md) ‚Ä¢ [Lecture 17 >>](lecture_17.md)
## Lecture 16 - Independent Component Analysis & RL

[![Lecture 16 - Independent Component Analysis & RL | Stanford CS229: Machine Learning (Autumn 2018)](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DYQA9lLdLig8%26list%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU%26index%3D16)](https://www.youtube.com/watch?v=YQA9lLdLig8&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=16)

### Overview

### Timestamps
  
[5:29](https://youtu.be/YQA9lLdLig8?si=nHUlzyfvjJTwbvVV&t=329): üîÄ Ambiguity in Independent Component Analysis due to rotational symmetry of Gaussian distribution makes it impossible to recover original sources.  
[12:08](https://youtu.be/YQA9lLdLig8?si=nHUlzyfvjJTwbvVV&t=728): ‚öôÔ∏è Deriving maximum likelihood estimate for parameters in the model of X = A_s = w inverse of s, and s = w_x.  
[18:37](https://youtu.be/YQA9lLdLig8?si=nHUlzyfvjJTwbvVV&t=1117): üìä Non-Gaussian distribution choice affects the shape of the PDF, with sigmoid function inducing fatter tails than Gaussian density.  
[26:10](https://youtu.be/YQA9lLdLig8?si=nHUlzyfvjJTwbvVV&t=1570): üîë Unmixing sources using ICA algorithm with stochastic gradient ascent to maximize log-likelihood.  
[33:15](https://youtu.be/YQA9lLdLig8?si=nHUlzyfvjJTwbvVV&t=1995): üß† Exploration of unsupervised learning and potential in EEG data cleaning with ICA.  
[38:19](https://youtu.be/YQA9lLdLig8?si=nHUlzyfvjJTwbvVV&t=2299): üß† Brain activity can be categorized using EEG readings and cleaned data to train learning algorithms for coarse level thoughts.  
[43:52](https://youtu.be/YQA9lLdLig8?si=nHUlzyfvjJTwbvVV&t=2632): üîç Complex concepts in independent component analysis and maximum likelihood model are discussed, including the challenges of separating overlapping voices with one microphone.  
[49:15](https://youtu.be/YQA9lLdLig8?si=nHUlzyfvjJTwbvVV&t=2955): ü§ñ Introduction to reinforcement learning and its application in training a helicopter to fly.  
[54:15](https://youtu.be/YQA9lLdLig8?si=nHUlzyfvjJTwbvVV&t=3255): üé≤ The delayed impact of early mistakes in chess and self-driving cars raises questions about algorithm learning and decision-making.  
[1:00:54](https://youtu.be/YQA9lLdLig8?si=nHUlzyfvjJTwbvVV&t=3654): ü§ñ Modeling Noisy Dynamics of Real Robots  
[1:07:35](https://youtu.be/YQA9lLdLig8?si=nHUlzyfvjJTwbvVV&t=4055): ‚è≥ Discount factor reduces weight of distant future rewards, encouraging faster reward deposition. In financial applications, it represents the time value of money.  
[1:13:59](https://youtu.be/YQA9lLdLig8?si=nHUlzyfvjJTwbvVV&t=4439): üéì Reinforcement Learning in MDPs maximizes expected payoff by finding optimal policy for given problem.  

Timestamps by Tammy AI

[<< Lecture 15](lecture_15.md) ‚Ä¢ [Lecture 17 >>](lecture_17.md)