[<< Lecture 9](lecture_9.md) • [Lecture 11 >>](lecture_11.md)
## Lecture 10 - Decision Trees and Ensemble Methods

[![Lecture 10 - Decision Trees and Ensemble Methods | Stanford CS229: Machine Learning (Autumn 2018)](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dwr9gUr-eWdA%26list%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU%26index%3D10)](https://www.youtube.com/watch?v=wr9gUr-eWdA&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=10)

### Overview

### Timestamps
  
[0:27](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=27): 🎓 Raphael Townshend gives a lecture on decision trees and ensemble methods.  
[6:00](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=360): 📚 The video discusses the concept of splitting regions and defining a split function in machine learning.  
[15:06](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=906): 📊 The video discusses the limitations of misclassification loss and introduces cross-entropy loss as a more sensitive alternative.  
[21:00](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=1260): 📊 The video discusses the concept of misclassification loss and cross-entropy loss in binary classification problems and provides an intuitive explanation for their differences.  
[27:23](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=1643): 📉 The video discusses the shape of loss curves in machine learning models.  
[35:29](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=2129): 🌍 The video discusses the concept of splitting data based on categories and subsets.  
[43:57](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=2637): 🌳 The video explains the runtime and training process of decision trees.  
[51:19](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=3079): 🌳 Ensembling can improve the performance of decision trees and is commonly used in Kaggle competitions.  
[1:00:59](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=3659): 🎒 Bagging, or bootstrap aggregation, is a method used in statistics to measure the uncertainty of an estimate by drawing multiple training sets from a population.  
[1:07:48](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=4068): ⚙️ Bootstrapping can decrease variance but slightly increase bias in models.  
[1:15:09](https://youtu.be/wr9gUr-eWdA?si=kgAbjzxSdrnXlSfX&t=4509): 💼 Boosting is a technique that decreases the bias of models by adding predictions from previous models into the ensemble.

Timestamps by Tammy AI

[<< Lecture 9](lecture_9.md) • [Lecture 11 >>](lecture_11.md)