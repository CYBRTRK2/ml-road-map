[<< Lecture 18](lecture_18.md) ‚Ä¢ [Lecture 20 >>](lecture_20.md)
## Lecture 19 - Reward Model and Linear Dynamical System

[![Lecture 19 - Reward Model & Linear Dynamical System | Stanford CS229: Machine Learning (Autumn 2018)](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D0rt2CsEQv6U%26list%3DPLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU%26index%3D19)](https://www.youtube.com/watch?v=0rt2CsEQv6U&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=19)

### Overview

* State-Action Rewards
* Finite Horizon Markov Decision Processes
* Linear Dynamic Systems

### Timestamps
  
[5:30](https://youtu.be/0rt2CsEQv6U?si=sGaBJFhKyCfQsQz2&t=330): ‚öôÔ∏è Reward modeling and action impact on Bellman's equation.  
[12:52](https://youtu.be/0rt2CsEQv6U?si=sGaBJFhKyCfQsQz2&t=772): ‚è≥ Finite horizon MDP involves maximizing rewards within a set time frame, with actions depending on the current time.  
[19:16](https://youtu.be/0rt2CsEQv6U?si=sGaBJFhKyCfQsQz2&t=1156): üéì Dynamic changes in state transition probability related to weight, weather forecasts, and industrial automation.  
[26:49](https://youtu.be/0rt2CsEQv6U?si=sGaBJFhKyCfQsQz2&t=1609): ‚è≥ Non-Stationary Policy Optimization  
[34:17](https://youtu.be/0rt2CsEQv6U?si=sGaBJFhKyCfQsQz2&t=2057): üìö Notation overload due to merging of control and CS literature, noise term assumed Gaussian with mean 0 and covariance sigma w.  
[42:33](https://youtu.be/0rt2CsEQv6U?si=sGaBJFhKyCfQsQz2&t=2553): üöÅ Helicopter dynamics model, methods of learning and linearization.  
[50:00](https://youtu.be/0rt2CsEQv6U?si=sGaBJFhKyCfQsQz2&t=3000): üìä Linear approximation of a function around a point in state-action space for inverted pendulum system.  
[57:54](https://youtu.be/0rt2CsEQv6U?si=sGaBJFhKyCfQsQz2&t=3474): üß† Optimal action is to choose 0 at the last time step due to positive semi-definite matrix V.  
[1:07:02](https://youtu.be/0rt2CsEQv6U?si=sGaBJFhKyCfQsQz2&t=4022): ‚öôÔ∏è Linear Optimal Action in Dynamical Systems  
[1:15:44](https://youtu.be/0rt2CsEQv6U?si=sGaBJFhKyCfQsQz2&t=4544): üß† Insight into LQR formula and its dependence on Phi but not Psi for computing optimal policy.  

Timestamps by Tammy AI

[<< Lecture 18](lecture_18.md) ‚Ä¢ [Lecture 20 >>](lecture_20.md)